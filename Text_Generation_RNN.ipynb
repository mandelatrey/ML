{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQRCVqGH0pFPEgxLq50QN/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mandelatrey/ML/blob/main/Text_Generation_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing modules"
      ],
      "metadata": {
        "id": "HED6tfk7_2XC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "71-w97jOT7Rl"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the dataset and finding text properties"
      ],
      "metadata": {
        "id": "OxKbz6XmWtkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/The Mysterious Island.txt', 'r', encoding='utf8') as fp:\n",
        "    text=fp.read()\n",
        "\n",
        "start_idx = text.find('THE MYSTERIOUS ISLAND')\n",
        "end_idx = text.find('End of the Project Gutenberg')\n",
        "text = text[start_idx:end_idx]\n",
        "char_set = set(text)\n",
        "print('Total length: ', len(text))\n",
        "print('Unique Chars: ', len(char_set))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPXEgRCUUjfY",
        "outputId": "c51f2bfe-b263-45ee-e4ae-de87ca213ef0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total length:  1130711\n",
            "Unique Chars:  85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we then build a dictionary and map chars to integers"
      ],
      "metadata": {
        "id": "30xcKaTaWniu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_sorted = sorted(char_set)\n",
        "char2int = {ch:i for i, ch in enumerate(chars_sorted)}\n",
        "char_array = np.array(chars_sorted)\n",
        "text_encoded = np.array(\n",
        "    [char2int[ch] for ch in text],\n",
        "    dtype=np.int32\n",
        ")\n",
        "\n",
        "print('text encoded shape: ', text_encoded.shape)\n",
        "print(text[:15], '== Encoding ==> ', text_encoded[:15]) #first 15, last 15\n",
        "print(text_encoded[15:21], '==reversed==>',\n",
        "      ''.join(char_array[text_encoded[15:21]]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cgw4BgTFVVRf",
        "outputId": "75e90395-b314-4b26-d4ce-ea441c29dd79"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text encoded shape:  (1130711,)\n",
            "THE MYSTERIOUS  == Encoding ==>  [48 36 33  1 41 53 47 48 33 46 37 43 49 47  1]\n",
            "[37 47 40 29 42 32] ==reversed==> ISLAND\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualising the encodings"
      ],
      "metadata": {
        "id": "UOXbOypcADZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ex in text_encoded[:15]:\n",
        "    print('{} --> {}'.format(ex, char_array[ex]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EreN3_G2WenC",
        "outputId": "3d28779d-38da-4b66-8fc8-b63dcc639ee2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48 --> T\n",
            "36 --> H\n",
            "33 --> E\n",
            "1 -->  \n",
            "41 --> M\n",
            "53 --> Y\n",
            "47 --> S\n",
            "48 --> T\n",
            "33 --> E\n",
            "46 --> R\n",
            "37 --> I\n",
            "43 --> O\n",
            "49 --> U\n",
            "47 --> S\n",
            "1 -->  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "seq_length = 40\n",
        "chunk_size = seq_length + 1\n",
        "text_chunks = [text_encoded[i:i+chunk_size]\n",
        "               for i in range(len(text_encoded)-chunk_size)]\n",
        "               \n",
        "from torch.utils.data import Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_chunks):\n",
        "        self.text_chunks = text_chunks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_chunks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text_chunk = self.text_chunks[idx]\n",
        "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
        "\n",
        "seq_dataset = TextDataset(torch.tensor(text_chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H9nF4qxXBIZ",
        "outputId": "1dd9d4d6-774b-457c-c009-a7264662e91d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-82f15c289c68>:21: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  seq_dataset = TextDataset(torch.tensor(text_chunks))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, (seq, target) in enumerate(seq_dataset):\n",
        "#     print(' input (x): ',\n",
        "#           repr(''.join(char_array[seq])))\n",
        "#     print('Target (y): ',\n",
        "#           repr(''.join(char_array[target])))\n",
        "#     print()\n",
        "#     if i == 1:\n",
        "#         break\n",
        "\n",
        "\n",
        "for i, (seq, target) in enumerate(seq_dataset):\n",
        "    print(' Input (x): ',\n",
        "        repr(''.join(char_array[seq])))\n",
        "    print('Target (y): ',\n",
        "        repr(''.join(char_array[target])))\n",
        "    print()\n",
        "    if i == 1:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oalvFf56Zo7C",
        "outputId": "37aff1ea-da1e-49bb-db57-5570664f7d7b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Input (x):  'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTER'\n",
            "Target (y):  'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
            "\n",
            " Input (x):  'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
            "Target (y):  'E MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERIO'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "torch.manual_seed(1)\n",
        "seq_dl = DataLoader(seq_dataset, batch_size=batch_size,\n",
        "                    shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "G8YZmpAYaw9t"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "dpbCG7_zb8Jd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a model"
      ],
      "metadata": {
        "id": "SoqcAO1WAQMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n",
        "                           batch_first=True)\n",
        "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embedding(x).unsqueeze(1)\n",
        "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
        "        out = self.fc(out).reshape(out.size(0), -1)\n",
        "        return out, hidden, cell\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        return hidden, cell"
      ],
      "metadata": {
        "id": "ksNNmhgtbpQ2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(char_array)\n",
        "embed_dim = 256\n",
        "rnn_hidden_size = 512\n",
        "torch.manual_seed(1)\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
        "model\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdqD5cJIcLsE",
        "outputId": "005ae9f6-e06b-4f1b-ffd1-5c0d950b2f6b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(85, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=85, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making a loss function"
      ],
      "metadata": {
        "id": "vt1NW7QtATLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "Qe8Xc3UZcMqB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then train the model"
      ],
      "metadata": {
        "id": "_ryN1AsgAZWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10000\n",
        "torch.manual_seed(1)\n",
        "for epoch in range(num_epochs):\n",
        "    hidden, cell = model.init_hidden(batch_size)\n",
        "    seq_batch, target_batch = next(iter(seq_dl))\n",
        "    optimiser.zero_grad()\n",
        "    loss = 0\n",
        "    for c in range(seq_length):\n",
        "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
        "        loss += loss_fn(pred, target_batch[:, c])\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    loss = loss.item()/seq_length\n",
        "\n",
        "    if epoch % 300 == 0:\n",
        "        print(f\"Epoch {epoch} loss: {loss:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAV9ayBLcMnq",
        "outputId": "38495889-ab91-4cef-a737-3ed9d57d67bb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 4.4368\n",
            "Epoch 300 loss: 1.6249\n",
            "Epoch 600 loss: 1.4520\n",
            "Epoch 900 loss: 1.3720\n",
            "Epoch 1200 loss: 1.3754\n",
            "Epoch 1500 loss: 1.3374\n",
            "Epoch 1800 loss: 1.2882\n",
            "Epoch 2100 loss: 1.2668\n",
            "Epoch 2400 loss: 1.2464\n",
            "Epoch 2700 loss: 1.1897\n",
            "Epoch 3000 loss: 1.1967\n",
            "Epoch 3300 loss: 1.1585\n",
            "Epoch 3600 loss: 1.1674\n",
            "Epoch 3900 loss: 1.1354\n",
            "Epoch 4200 loss: 1.1324\n",
            "Epoch 4500 loss: 1.1363\n",
            "Epoch 4800 loss: 1.1005\n",
            "Epoch 5100 loss: 1.1010\n",
            "Epoch 5400 loss: 1.1293\n",
            "Epoch 5700 loss: 1.1394\n",
            "Epoch 6000 loss: 1.1279\n",
            "Epoch 6300 loss: 1.1001\n",
            "Epoch 6600 loss: 1.0714\n",
            "Epoch 6900 loss: 1.0783\n",
            "Epoch 7200 loss: 1.0575\n",
            "Epoch 7500 loss: 1.0715\n",
            "Epoch 7800 loss: 1.0046\n",
            "Epoch 8100 loss: 1.0540\n",
            "Epoch 8400 loss: 1.0322\n",
            "Epoch 8700 loss: 1.0152\n",
            "Epoch 9000 loss: 0.9880\n",
            "Epoch 9300 loss: 1.0865\n",
            "Epoch 9600 loss: 0.9451\n",
            "Epoch 9900 loss: 0.9871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use categoricals to sample random samples from the distributn"
      ],
      "metadata": {
        "id": "Ghun5Zw4Alyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions.categorical import Categorical"
      ],
      "metadata": {
        "id": "FQzem2dP5miN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, starting_str,\n",
        "    len_generated_text=500,\n",
        "    scale_factor=1.0):\n",
        "    encoded_input = torch.tensor(\n",
        "        [char2int[s] for s in starting_str]\n",
        "    )\n",
        "    encoded_input = torch.reshape(\n",
        "        encoded_input, (1, -1)\n",
        "    )\n",
        "    generated_str = starting_str #first set it to input\n",
        "\n",
        "    model.eval()\n",
        "    hidden, cell = model.init_hidden(1)\n",
        "    for c in range(len(starting_str)-1):\n",
        "        _, hidden, cell = model(\n",
        "            encoded_input[:, c].view(1), hidden, cell\n",
        "        )\n",
        "\n",
        "    last_char = encoded_input[:, -1]\n",
        "    for i in range(len_generated_text):\n",
        "        logits, hidden, cell = model(\n",
        "            last_char.view(1), hidden, cell\n",
        "        )\n",
        "\n",
        "        logits = torch.squeeze(logits, 0)\n",
        "        scaled_logits = logits * scale_factor\n",
        "        m = Categorical(logits=scaled_logits)\n",
        "        last_char = m.sample()\n",
        "        generated_str += str(char_array[last_char])\n",
        "\n",
        "    return generated_str\n",
        "\n",
        "torch.manual_seed(53)\n",
        "print(sample(model, starting_str='The boys stood amazed at', \n",
        "             scale_factor=2.0)) # 2.0=more predictable 0.5=more random"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFzksY-7iEG6",
        "outputId": "0054eefa-a1ad-4b32-cb92-f6fdb7b87533"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The boys stood amazed at the corral, and to watch the wind spiring with the first sight of the rocks. They must have\n",
            "reasoning to the beach, and the true, we shall see a dozen of the forest, and\n",
            "the sailor talked away the water to the east, and the new crater and the situation of the\n",
            "distance in the sand.”\n",
            "\n",
            "“We shall see that the time when the colonists,” replied Cyrus Harding.\n",
            "\n",
            "“And that we can go some day that the convulsion of this storm might be the passages and transformed into the surface of the lake. It was imag\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wFYNljks-YBs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}